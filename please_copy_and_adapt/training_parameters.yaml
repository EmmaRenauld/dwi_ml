#################
#  Parameters
#
# Here is a choice of parameters for which verification is already coded in
# dwi_ml. You can comment/delete parameters that are not necessary for your
# project, and add new parameters.
#
# In train_model.py, you should adapt the script to define which parameters are
# required or optional.
#
# Note: In most cases, you can keep the variable and set it to ~, to null or to
#       an empty value (all are yaml's equivalent to python's None).
# Tip:  To set a boolean in yaml, write either True/False or On/Off. Both
#       choices correspond to python's True/False.
##################

logging:
  # level: one of ['error', 'warning', 'info', 'debug']
  #     Logging level. If null, default = warning.
  level: debug

preprocessing:
  # step_size: float
  #     Resample all streamlines to this step size (in mm). If null, train on
  #     streamlines as they are (e.g. compressed).
  step_size: null
  # normalize_directions: bool
  #     If true, directions will be normalized. If the step size is fixed, it
  #     shouldn't make any difference. If streamlines are compressed, in theory
  #     you should normalize, but you could hope that not normalizing could
  #     give back to the algorithm a sense of distance between points.
  normalize_directions: True

data_augmentation:
  # noise_size: bool
  #     Add random Gaussian noise to streamline coordinates with given
  #     variance. This corresponds to the std of the Gaussian. If step_size is
  #     not given, make sure it is smaller than your step size to avoid
  #     flipping direction. Ex, you could choose 0.1 * step-size. Noise is
  #     truncated to +/- 2*noise_sigma and to +/- 0.5 * step-size (if given).
  #     null or 0 both lead to no added noise.
  noise_size: 0.
  # noise_variability: bool
  #     If this is given, a variation is applied to the noise_size to have very
  #     noisy streamlines and less noisy streamlines. This means that the real
  #     gaussian_size will be a random number between
  #     [size - variability, size + variability]. null is the same as 0.
  noise_variability: null
  # split_ratio: float
  #     Percentage of streamlines to randomly split into 2, in each batch
  #     (keeping both segments as two independent streamlines). The reason for
  #     cutting is to help the ML algorithm to track from the middle of WM by
  #     having already seen half-streamlines. If you are using interface
  #     seeding, this is not necessary.
  split_ratio: null
  # reverse_ratio: float
  #     Percentage of streamlines to randomly reverse in each batch.
  reverse_ratio: null

input:
  neighborhood:
    # sphere_radius: float
    #     If not null, a neighborhood will be added to the input information.
    #     This neighborhood definition lies on a sphere. It will be a list of 6
    #     positions (up, down, left, right, behind, in front) at exactly given
    #     radius around each point of the streamlines.
    #     * Can't be used together with grid_radius.
    sphere_radius: null
    # grid_radius: int
    #     If not null, a neighborhood will be added to the input information.
    #     This neighborhood definition uses a list of points similar to the
    #     original voxel grid around each point of the streamlines. Ex: with
    #     radius 1, that's 27 points. With radius 2, that's 125 points.
    #     * Can't be used together with sphere_radius.
    grid_radius: null
  # num_previous_dirs: int
  #     Concatenate X previous streamline directions to the input vector. Null
  #     is equivalent to 0.
  num_previous_dirs: null

model:
  # Add your model's parameters

training:
  epochs:
    # max_epochs: int
    #     Maximum number of epochs. Suggestion: 100.
    max_epochs: 2
    # patience: int
    #     Use early stopping. Defines the number of epochs after which the
    #     model should stop if the loss hasn't improved. Suggestion: 20. If
    #     null, the model will continue for max_epochs.
    patience: null
  batch:
    # batch_size: int
    #     Number of streamline points per batch. Suggestion: 20000.
    #     NOTE: Actual batch sizes might be different than `batch_size`
    #     depending on chosen data augmentation.
    size: 20
    # n_subjects_per_batch: int
    #     Maximum number of different subjects from which to load data in each
    #     batch. This should help avoid loading too many inputs in memory,
    #     particularly for lazy data. If null, we will use true random sampling.
    #     Suggestion: 5.
    #     Hint: Will influence the cache size if memory:cache_manager is used.
    n_subjects_per_batch: 1
    # cycles: int
    #     Relevant only if training:batch:n_subject_per_batch is not null.
    #     Number of cycles before changing to new subjects (and thus new
    #     volumes). null is equivalent to 1.
    cycles: 2

memory:
  # lazy: bool
  #     If set, do not load all the dataset in memory at once. Load only what
  #     is needed for a batch.
  lazy: False
  # cache_manager: bool
  #     Relevant only if memory:lazy is used. If set, will cache volumes and
  #     streamlines in memory instead of fetching from the disk everytime.
  #     Cache size will depend on epochs:volumes_per_batch.
  cache_manager: False
  # avoid_cpu_computations: bool
  #     If set, all possible computations that can be avoided on cpu will be
  #     skipped in the batch sampler. Ex: interpolation, directions
  #     computations from the streamlines coordinates, etc.
  avoid_cpu_computations: True
  # num_cpu_workers: int
  #     Number of parallel CPU workers.
  num_cpu_workers: null
  # worker_interpolation: bool
  #     If set, if using num_cpu_workers > 0, interpolation will be done on CPU
  #     by the workers instead of on the main thread using the chosen device.
  worker_interpolation: False
  # taskman_managed: bool
  #     If set, instead of printing progression, print taskman-relevant data.
  taskman_managed: False

randomization:
  # rng: int
  #     Random experiment seed.
  rng: 1234
